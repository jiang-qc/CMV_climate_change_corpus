{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFluaToijPmE"
   },
   "source": [
    "# Extract Sentences\n",
    "\n",
    "`COLX_523_Group7/env`: work in the local `virtualenv`!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "izjwpyTOxsqI",
    "outputId": "5e06165b-36d8-4cf7-9456-f98165f50950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/COLX_523')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF3wV8dXDAcB"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "yYoHwz7iyAGc"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os.path as path\n",
    "import csv\n",
    "import random, os, json, csv\n",
    "random.seed(523)\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True) | nltk.download('punkt', quiet=True)\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "wNPy4vQ0QtPz",
    "outputId": "f7528c52-df24-4453-a0de-4af68e8476a3"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VRyMteOK0k21"
   },
   "outputs": [],
   "source": [
    "def load_master(master_dir):\n",
    "    \"\"\"Returns three master files present in `master_dir`. If such directory does\n",
    "    not exist, create one and three master files, and then return them. \n",
    "    \"\"\"    \n",
    "    with open(os.path.join(master_dir, 'submission.json'), 'r', encoding='utf-8') as f:\n",
    "        submission_dict = json.load(f)\n",
    "    with open(os.path.join(master_dir, 'delta_thread.json'), 'r', encoding='utf-8') as f:\n",
    "        delta_thread_dict = json.load(f)\n",
    "    with open(os.path.join(master_dir, 'log.json'), 'r') as f:\n",
    "        log_dict = json.load(f)\n",
    "\n",
    "    return submission_dict, delta_thread_dict, log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6TvtvEXrygXY"
   },
   "outputs": [],
   "source": [
    "MASTER_DIR = '../src/reddit_scraper/reddit_corpus_mar5'  #'feb26'\n",
    "submission_dict, delta_thread_dict, log_dict = load_master(MASTER_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzHgO5pB1SG-"
   },
   "source": [
    "# Extract `climate change` submissions and threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-2u5Vhb0mp7",
    "outputId": "e04cd5c9-64e7-44d5-f345-b8cf4bedc7de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 topics 117 threads\n"
     ]
    }
   ],
   "source": [
    "KEYWORD = 'climate change'\n",
    "s_df = pd.DataFrame(submission_dict)\n",
    "cc_idx = s_df[s_df['submission_title'].str.contains(KEYWORD, case=False)].index\n",
    "cc_submission_ids = s_df.loc[cc_idx]['submission_id']\n",
    "#print(cc_submission_ids.values)  # f0y5af  g0tkib  p7h645\n",
    "cc_thread = []\n",
    "for key in delta_thread_dict:\n",
    "    s_id, _ = key.split('-')\n",
    "    if s_id in cc_submission_ids.values:\n",
    "        cc_thread.append(key)\n",
    "print(len(cc_submission_ids), 'topics', len(cc_thread), 'threads')  # 15 (feb26)  --> 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GNjf8TB5yAJ9"
   },
   "outputs": [],
   "source": [
    "def get_submission_title(submission_id):\n",
    "    idx = submission_dict['submission_id'].index(submission_id)\n",
    "    return submission_dict['submission_title'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mRhy90SXyS1R"
   },
   "outputs": [],
   "source": [
    "def get_submission_body(submission_id):\n",
    "    idx = submission_dict['submission_id'].index(submission_id)\n",
    "    return submission_dict['submission_body'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVr53h2XszuI",
    "outputId": "914d8f98-288e-498f-a341-e5d812108a5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_id': 'fgzuyrk',\n",
       " 'created_utc': 1581197912.0,\n",
       " 'timestamp': '2020-02-08 21:38:32',\n",
       " 'author': 'yyzjertl',\n",
       " 'parent_id': 't3_f0y5af',\n",
       " 'comment': 'I mean, I know several Trump supporters who believe climate change is happening and is human caused, and think what Trump says about this issue is technically wrong.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = delta_thread_dict['f0y5af-fh0f5n6'][0]\n",
    "comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_y-udjFbszx_",
    "outputId": "820ce508-dfd2-4b1c-cf0e-00b240aec5a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I mean, I know several Trump supporters who believe climate change is happening and is human caused, and think what Trump says about this issue is technically wrong.\n",
      "I mean, I know several Trump supporters who believe climate change is happening and is human caused, and think what Trump says about this issue is technically wrong.\n",
      "['I', 'mean,', 'I', 'know', 'several', 'Trump', 'supporters', 'who', 'believe', 'climate', 'change', 'is', 'happening', 'and', 'is', 'human', 'caused,', 'and', 'think', 'what', 'Trump', 'says', 'about', 'this', 'issue', 'is', 'technically', 'wrong.']\n",
      "31 ['I', 'mean', ',', 'I', 'know', 'several', 'Trump', 'supporters', 'who', 'believe', 'climate', 'change', 'is', 'happening', 'and', 'is', 'human', 'caused', ',', 'and', 'think', 'what', 'Trump', 'says', 'about', 'this', 'issue', 'is', 'technically', 'wrong', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(comment['comment'])\n",
    "for sent in doc.sents:\n",
    "    print(sent)  # Span object\n",
    "    print(sent.text)  # str\n",
    "    print(sent.text.split())  # simple split\n",
    "    nltk = word_tokenize(sent.text)   # NLTK tokenizer\n",
    "    print(len(nltk), nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "nltk = word_tokenize(comment['comment'])\n",
    "print(len(word_tokenize(sent.text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word counts of climate change texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_tokens(submission_id):\n",
    "    text = get_submission_title(submission_id)\n",
    "    sents = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    return sents, words\n",
    "\n",
    "def get_body_tokens(submission_id):\n",
    "    text = get_submission_body(submission_id)\n",
    "    sents = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    return sents, words\n",
    "\n",
    "def count_corpus(submission_ids, thread_ids):\n",
    "    sub_sent_count, sub_word_count = 0, 0  # submissions\n",
    "    thr_sent_count, thr_word_count = 0, 0  # threads\n",
    "\n",
    "    for submission_id in submission_ids:\n",
    "        sents, words = get_title_tokens(submission_id)\n",
    "        sub_sent_count += len(sents)\n",
    "        sub_word_count += len(words)\n",
    "        sents, words = get_body_tokens(submission_id)\n",
    "        sub_sent_count += len(sents)\n",
    "        sub_word_count += len(words)\n",
    "    print('Submissions:', sub_sent_count, 'sentences', sub_word_count, 'words')\n",
    "\n",
    "    for thread_id in thread_ids:\n",
    "        for comment_dict in delta_thread_dict[thread_id]:\n",
    "            comment = comment_dict['comment'].replace('\\n\\n', ' ')\n",
    "            sents = sent_tokenize(comment)\n",
    "            words = word_tokenize(comment)\n",
    "            thr_sent_count += len(sents)\n",
    "            thr_word_count += len(words)\n",
    "    print('Threads:', thr_sent_count, 'sentences', thr_word_count, 'words')\n",
    "    print('-'*50, )\n",
    "    print('Total:', sub_sent_count+thr_sent_count, 'sentences', sub_word_count+thr_word_count, 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submissions: 45322 sentences 997521 words\n",
      "Threads: 62386 sentences 1238032 words\n",
      "--------------------------------------------------\n",
      "Total: 107708 sentences 2235553 words\n"
     ]
    }
   ],
   "source": [
    "# Entire corpus\n",
    "count_corpus(submission_dict['submission_id'], delta_thread_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submissions: 1354 sentences 32338 words\n",
      "Threads: 3170 sentences 65826 words\n",
      "--------------------------------------------------\n",
      "Total: 4524 sentences 98164 words\n"
     ]
    }
   ],
   "source": [
    "# Climate Change text\n",
    "count_corpus(cc_submission_ids, cc_thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfIdf\n",
    "https://www.geeksforgeeks.org/tf-idf-for-bigrams-trigrams/\n",
    "\n",
    "https://qiita.com/fujin/items/b1a7152c2ec2b4963160\n",
    "\n",
    "1. Create a list of word tokenized sentences\n",
    "2. Preprocessing & stopwords removal\n",
    "3. Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_words(sent):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    invalid_words = ['cmv', '&#x200B;', 'delta!', 'xb', '∆', ':)']\n",
    "    return ' '.join([x for \n",
    "            x in nltk.word_tokenize(sent) if \n",
    "            (x not in stop_words) and (x not in invalid_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def remove_string_special_characters(sent):\n",
    "    # removes special characters with ' '\n",
    "    stripped = re.sub('[^a-zA-z\\s]', '', sent)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "      \n",
    "    # Change any white space to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "    if stripped != '':\n",
    "        return stripped.lower()\n",
    "    else:\n",
    "#        print(\"NONE!!!\", sent)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_text(text):\n",
    "    tokenized = []\n",
    "    clean = text.replace('\\n\\n', ' ')\n",
    "    try:\n",
    "        doc = nlp(clean)\n",
    "        for sent in doc.sents:\n",
    "            clean = remove_string_special_characters(sent.text)\n",
    "            if clean:\n",
    "                clean = remove_invalid_words(clean)\n",
    "            if clean:\n",
    "                tokenized.append(clean)\n",
    "        return tokenized\n",
    "    except Exception as e:\n",
    "        return None        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 sentences in 76 submissions\n"
     ]
    }
   ],
   "source": [
    "txt1 = []  # title\n",
    "for submission_id in cc_submission_ids.values:\n",
    "    text = get_submission_title(submission_id)\n",
    "    txt1.extend(clean_up_text(text))   \n",
    "print(len(txt1), 'sentences in', len(cc_submission_ids), 'submissions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287 sentences in 76 submission bodies\n"
     ]
    }
   ],
   "source": [
    "txt2 = []  # body\n",
    "for submission_id in cc_submission_ids.values:\n",
    "    text = get_submission_body(submission_id)\n",
    "    txt2.extend(clean_up_text(text))\n",
    "print(len(txt2), 'sentences in', len(cc_submission_ids), 'submission bodies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3227 sentences in 117 threads\n"
     ]
    }
   ],
   "source": [
    "txt3 = []  # threads\n",
    "for thread_id in cc_thread:\n",
    "    for comment_dict in delta_thread_dict[thread_id]:\n",
    "        text = comment_dict['comment']\n",
    "        txt3.extend(clean_up_text(text))   \n",
    "print(len(txt3), 'sentences in', len(cc_thread), 'threads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4597"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_txt = txt1 + txt2 + txt3\n",
    "len(all_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigrams & Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3660, 2) \n",
      "                 term        rank\n",
      "455           change  105.214976\n",
      "544          climate  100.230036\n",
      "547   climate change   86.551904\n",
      "2334          people   78.095081\n",
      "3598           would   67.411036\n",
      "3264           think   59.727931\n",
      "907             dont   53.041031\n",
      "1867            like   49.614273\n",
      "814            delta   49.576591\n",
      "2248             one   41.960582\n",
      "1083            even   36.553922\n",
      "2428           point   36.011167\n",
      "3238           thats   35.629433\n",
      "1365             get   35.572902\n",
      "3580           world   35.391770\n",
      "116             also   35.147767\n",
      "2118            much   34.725528\n",
      "1030          energy   33.792855\n",
      "3418              us   32.780142\n",
      "1961            make   31.957445\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3)\n",
    "X = vectorizer.fit_transform(all_txt)\n",
    "features = (vectorizer.get_feature_names())\n",
    "sums = X.sum(axis=0)\n",
    "data = []\n",
    "for column, term in enumerate(features):\n",
    "    data.append((term, sums[0, column]))\n",
    "ranking = pd.DataFrame(data, columns=['term', 'rank'])\n",
    "ranked_words = ranking.sort_values('rank', ascending=False)\n",
    "print(ranked_words.shape, '\\n', ranked_words.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save on the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_result.txt', 'w') as f:\n",
    "    count = 0\n",
    "    for term, rank in zip(ranked_words['term'][count:], ranked_words['rank'][count:]):\n",
    "        f.write(f'{count}\\t{term}\\t{round(rank, 2)}\\n')\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83aG_wuPralt"
   },
   "source": [
    "# Extract the sentences for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Extract all sentences from the climate exchange topics\n",
    "# 2. Iterate over the list of \"climate change\" thread_ids\n",
    "# 3. For each comment under the thread, apply spaCy tokenizer (comment -> sentence)\n",
    "# 4. Add the tokenized comment (a set of sentences) to CSV, along with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_words(sent):\n",
    "    invalid_words = ['cmv', '&#x200B;', 'delta!', 'xb', '∆', ':)']\n",
    "    return ' '.join([x for \n",
    "            x in nltk.word_tokenize(sent) if \n",
    "            x not in invalid_words])\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(sent):\n",
    "\n",
    "    # Change any white space to one space\n",
    "    clean = re.sub('\\s+', ' ', sent).strip()\n",
    "      \n",
    "    # Remove start and end white spaces\n",
    "    try:\n",
    "        if clean != '':\n",
    "            clean = remove_invalid_words(clean)\n",
    "            if clean != '':\n",
    "                return clean\n",
    "    except:\n",
    "        print(\"NONE!!!\", sent)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_MUST = ['pollution', 'protest', 'responsible', 'green energy', 'emission', 'dishonest', \n",
    "            'change', 'issue', 'fact', 'control', 'viable', 'solve', 'health', 'compromise', \n",
    "            'order', 'answer', 'effect', 'measures', 'nuclear energy', 'disaster', \n",
    "            'actively deny', 'address', 'fossil fuel', 'responsibilit', 'global climate', \n",
    "            'worry', 'renewable', 'cause', 'problem', 'global', 'co2', 'society', \n",
    "            'carbon footprint', 'discuss', 'nuclear', 'solution', 'climate change', 'atmospher', \n",
    "            'prevent', 'ozone hole', 'carbon'\n",
    "           ]\n",
    "\n",
    "KEY_ASPECT = {\n",
    "    'politics': ['policy', 'developed countr', 'socialism', 'bill', 'developing countr', \n",
    "                 'politician', 'conservative', 'countr', 'politicize', 'government', \n",
    "                 'country', 'republican', 'right wing', 'democrat', 'state'\n",
    "    ],\n",
    "    'humanity': ['change myth', 'consumer', 'extinction', 'stewardship', 'students', 'wealth', \n",
    "                 'children', 'end human', 'extinct', 'human civilization', 'generation', \n",
    "                 'kids', 'mancaused', 'apocalypse', 'humanity', 'apocalyptic', 'existential threat', \n",
    "                 'luxuries', 'education', 'healthcare', 'lifestyle', 'overpopulation', 'citizen', \n",
    "                 'society', 'starvation', 'moral', 'selfish', 'everyday people', \n",
    "                 'individual responsibilit', 'individuals', 'survive', 'poverty', 'people', \n",
    "                 'manmade', 'human', 'people believe', 'activist', 'civilization', 'habitable', \n",
    "                 'spending'\n",
    "    ],\n",
    "    'science': ['academ', 'advancement', 'experiment', 'scientist', 'academia', \n",
    "                'technological', 'renewable', 'green', 'innovation', 'medical', 'technology', \n",
    "                'scientific', 'empirical', 'science', 'research', 'scholar'\n",
    "    ],\n",
    "    'industry': ['corporation', 'invest', 'ceo', 'market', 'ethic', 'industry', 'economy', \n",
    "                 'company', 'entity', 'profit'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def contain_must_key(sent):\n",
    "    flag = False\n",
    "    for keyword in KEY_MUST:\n",
    "        if keyword in sent:\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "def contain_aspect_key(sent, aspect):\n",
    "    flag = False\n",
    "    for keyword in KEY_ASPECT[aspect]:\n",
    "        if keyword in sent:\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "def contain_keyword(sent, aspect):\n",
    "    flag = False\n",
    "    if contain_must_key(sent):\n",
    "        if contain_aspect_key(sent, aspect):\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "def get_keyword(sent, aspect):\n",
    "    keywords = []\n",
    "    for keyword in KEY_MUST+KEY_ASPECT[aspect]:\n",
    "        if keyword in sent:\n",
    "            keywords.append(keyword)\n",
    "#    print(keywords)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialize selected_sent_ids (next cell)\n",
    "2. L7-8 --> choose 'w' or 'a'\n",
    "3. L10 --> comment off from the 2nd run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sent_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 sentences about industry\n",
      "722 sentences so far.\n"
     ]
    }
   ],
   "source": [
    "MIN, MAX = 15, 70 # a tweet 280 characters/avg 5 char per word = 56\n",
    "aspect_count = 0\n",
    "#ASPECT = 'politics'\n",
    "#ASPECT = 'humanity'\n",
    "#ASPECT = 'science'\n",
    "ASPECT = 'industry'\n",
    "#with open('climate_change_raw_v3.csv', 'w', newline='') as f:  # for 1st run\n",
    "with open('climate_change_raw_v3.csv', 'a', newline='') as f:  # for 2nd onwards\n",
    "    writer = csv.writer(f)\n",
    "#    writer.writerow(['id', 'title', 'focus_sentence', 'keywords', 'keyword_aspect', 'aspect_politics', 'aspect_humanity', 'aspect_science', 'aspect_economy'])  # comment off from the 2nd run!!\n",
    "    for thread_id in cc_thread:\n",
    "        submission_id, _ = thread_id.split('-')  # submissionID, delta_commentID\n",
    "        for comment_dict in delta_thread_dict[thread_id]:\n",
    "            comment_id = comment_dict['comment_id']\n",
    "            comment = comment_dict['comment'].replace('\\n\\n', ' ')\n",
    "            doc = nlp(comment)\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                selected_id = f'{thread_id}-{comment_id}-{sent_id}'\n",
    "                words = sent.text.split()\n",
    "                if (MIN <= len(words) < MAX) and (contain_keyword(sent.text, ASPECT)):\n",
    "                    if selected_id in set(selected_sent_ids):\n",
    "                        aspect_count += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        aspect_count += 1\n",
    "                        keywords = get_keyword(sent.text, ASPECT)\n",
    "                        writer.writerow([\n",
    "                                        selected_id, \n",
    "                                        get_submission_title(submission_id), \n",
    "                                        sent.text.strip(),\n",
    "                                        ', '.join(keywords),\n",
    "                                        ASPECT\n",
    "                        ])\n",
    "                        selected_sent_ids.append(selected_id)\n",
    "\n",
    "    for submission_id in cc_submission_ids.values:\n",
    "        submission_body = get_submission_body(submission_id)\n",
    "        doc = nlp(submission_body)\n",
    "        for sent_id, sent in enumerate(doc.sents):\n",
    "            selected_id = f'{submission_id}-{sent_id}'\n",
    "            words = sent.text.split()\n",
    "            if (MIN <= len(words) < MAX) and (contain_keyword(sent.text, ASPECT)):\n",
    "                if selected_id in set(selected_sent_ids):\n",
    "                    aspect_count += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    aspect_count += 1\n",
    "                    keywords = get_keyword(sent.text, ASPECT)\n",
    "                    writer.writerow([\n",
    "                                      selected_id,\n",
    "                                      get_submission_title(submission_id),\n",
    "                                      sent.text.strip(),\n",
    "                                      ', '.join(keywords),\n",
    "                                        ASPECT,\n",
    "                    ])\n",
    "                    selected_sent_ids.append(selected_id)\n",
    "print(aspect_count, \"sentences about\", ASPECT)\n",
    "print(len(selected_sent_ids), \"sentences so far.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tf-Idf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
